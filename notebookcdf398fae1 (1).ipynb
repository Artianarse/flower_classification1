{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Deep Learning with PyTorch for Flowers Classification**\n\n<p align=\"center\">\n  <img src=\"https://images.unsplash.com/photo-1601743875461-a4db41e22da5?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=1050&q=80\" width=\"60%\" />\n</p>\n\n**Objective:** In this project, we will build a deep learning classification model using CNNs, ResNet and regularization techniques to identify the flower species.\n\n**Dataset Source:** https://www.kaggle.com/alxmamaev/flowers-recognition/ - Version 2\n","metadata":{"id":"zzKIcQ5WS95c"}},{"cell_type":"markdown","source":"## **Downloading and Exploring the Data**\n\n**Downloading:** We can use the `opendatasets` library to download the [dataset](https://www.kaggle.com/alxmamaev/flowers-recognition) from Kaggle. `opendatasets` uses the [Kaggle Official API](https://github.com/Kaggle/kaggle-api) for downloading datasets from Kaggle. Follow these steps to find your API credentials:\n\n1. Sign in to [https://kaggle.com/](https://kaggle.com/), then click on your profile picture on the top right and select \"My Account\" from the menu.\n\n2. Scroll down to the \"API\" section and click \"Create New API Token\". This will download a file `kaggle.json` with the following contents: {\"username\":\"YOUR_KAGGLE_USERNAME\",\"key\":\"YOUR_KAGGLE_KEY\"}\n3. When you run `opendatsets.download`, you will be asked to enter your username & Kaggle API, which you can get from the file downloaded in step 2.\nNote that you need to download the `kaggle.json` file only once. On Google Colab, you can also upload the `kaggle.json` file using the files tab, and the credentials will be read automatically.","metadata":{"id":"zDrm7L2Vjur8"}},{"cell_type":"code","source":"!pip install opendatasets --upgrade --quiet","metadata":{"executionInfo":{"elapsed":3372,"status":"ok","timestamp":1610347388487,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"ujbTGU39jzzc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import opendatasets as od\n\ndataset_url = 'https://www.kaggle.com/alxmamaev/flowers-recognition/flowers'\nod.download(dataset_url)","metadata":{"executionInfo":{"elapsed":10597,"status":"ok","timestamp":1610347396181,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"NHJSTzrmlrxN","outputId":"1a247f4b-5bc6-4142-bf73-d13ea3e0ebd2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Exploring:** Here, we will get to know the dataset and clean it if necessary.","metadata":{"id":"OwpG-ctiy_cR"}},{"cell_type":"code","source":"import os\n\n# Look into the data directory\nroot_dir = './flowers-recognition'\ndata_dir = './flowers-recognition/flowers'\nprint(os.listdir(data_dir))","metadata":{"executionInfo":{"elapsed":6788,"status":"ok","timestamp":1610347396183,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"PuV_FuglvBUd","outputId":"e2deb71f-60e7-4152-ad05-c55bca6a64b8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There is a duplicate folder as _flowers_. Let's delete it.","metadata":{"id":"ZET15TgDviEz"}},{"cell_type":"code","source":"# Importing shutil library to delete the folder with it's contents\nimport shutil\n\n# Delete the duplicate folder\ndup_dir = data_dir + '/flowers'\nif os.path.exists(dup_dir) and os.path.isdir(dup_dir):\n    shutil.rmtree(dup_dir)\n\n# Look into the new data directory \nprint(os.listdir(data_dir))","metadata":{"executionInfo":{"elapsed":5239,"status":"ok","timestamp":1610347396185,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"a0R0PPHNuhHx","outputId":"ab0f5163-8073-46b5-9adb-3c2b7ea31f5f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's rename the images' file to include their class name.\n\n","metadata":{"id":"3EbJk_ay7IlH"}},{"cell_type":"code","source":"def rename_files(root_dir):\n    classes = os.listdir(root_dir)\n    for classes in classes:\n        for file in os.listdir(root_dir + '/' + classes): \n            if file.endswith('jpg'):\n                os.rename((root_dir + '/' + classes + '/' + file),(root_dir + '/' + classes + '/' + classes + \"_\" + file))\n\nrename_files(data_dir)","metadata":{"executionInfo":{"elapsed":4813,"status":"ok","timestamp":1610347396187,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"ZpT2p05Ewyu9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Preparing the Dataset**","metadata":{"id":"FSsf1O90VsIB"}},{"cell_type":"markdown","source":"**Custom dataset:** We can now create a custom dataset by extending the `Dataset` class from PyTorch. We need to define the `__len__` and `__getitem__` methods to create a dataset. We'll also provide the option of adding transforms into the constructor.\n\n","metadata":{"id":"AJJYfI-lY7Cr"}},{"cell_type":"code","source":"#import os\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\ndef parse_species(fname):\n    parts = fname.split('_')\n    return parts[0]\n\ndef open_image(path):\n    with open(path, 'rb') as f:\n        img = Image.open(f)\n        return img.convert('RGB')\n\nclass FlowersDataset(Dataset):\n    def __init__(self, root_dir, transform):\n        super().__init__()\n        self.root_dir = root_dir\n        self.files = []\n        self.classes = [fname for fname in os.listdir(root_dir) if fname != 'flowers']\n        for classes in self.classes:                         \n            for file in os.listdir(root_dir + '/' + classes): \n                if file.endswith('jpg'):\n                    self.files.append(file)\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, i):\n        fname = self.files[i]\n        species = parse_species(fname)\n        fpath = os.path.join(self.root_dir, species, fname)\n        img = self.transform(open_image(fpath))\n        class_idx = self.classes.index(species)\n        return img, class_idx","metadata":{"executionInfo":{"elapsed":7705,"status":"ok","timestamp":1610347399720,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"cHIjgdRbZCY8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Transforms can be chained using `transforms.Compose`.","metadata":{"id":"hAJ0UssmZZUu"}},{"cell_type":"code","source":"import torchvision.transforms as T\n\nimg_size = 64\nstats = ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\ntransform = T.Compose([T.Resize((img_size, img_size)),\n                       T.RandomCrop(64, padding=4, padding_mode='reflect'),\n                       T.RandomHorizontalFlip(),\n                       T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n                       T.ToTensor(),\n                       T.Normalize(*stats,inplace=True)])\ndataset = FlowersDataset(data_dir, transform=transform)","metadata":{"executionInfo":{"elapsed":6020,"status":"ok","timestamp":1610347399721,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"PXGB0wpnZ2L1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check how many samples the dataset contains\n","metadata":{"id":"QQf0MiSzZ3r9"}},{"cell_type":"code","source":"len(dataset)","metadata":{"executionInfo":{"elapsed":1157,"status":"ok","timestamp":1610347399722,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"lRDMvb7Mt42L","outputId":"bcda5f61-b938-47e6-afb4-8bb3a805b31b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at a sample image from the dataset. We'll define a function `show_image` to help us.\n\n","metadata":{"id":"sumZ_uHauBso"}},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef denormalize(images, means, stds):\n    if len(images.shape) == 3:\n        images = images.unsqueeze(0)\n    means = torch.tensor(means).reshape(1, 3, 1, 1)\n    stds = torch.tensor(stds).reshape(1, 3, 1, 1)\n    return images * stds + means\n\ndef show_image(img_tensor, label):\n    print('Label:', dataset.classes[label], '(' + str(label) + ')')\n    img_tensor = denormalize(img_tensor, *stats)[0].permute((1, 2, 0))\n    plt.imshow(img_tensor)","metadata":{"executionInfo":{"elapsed":529,"status":"ok","timestamp":1610347400064,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"PKXAqg9muHvO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's have a look at some images.","metadata":{"id":"JTMvTiSvuqMR"}},{"cell_type":"code","source":"show_image(*dataset[1746]);","metadata":{"executionInfo":{"elapsed":903,"status":"ok","timestamp":1610347401799,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"IFwo3gTQumr3","outputId":"5e2f2d0b-c6dc-4004-d4e6-f76c4b8d8726","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_image(*dataset[3745]);","metadata":{"executionInfo":{"elapsed":1071,"status":"ok","timestamp":1610347402779,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"PHmmATOSE9F1","outputId":"3b484844-a002-4109-c468-6974184144a5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training & Validation sets:** As a good practice, we should split the data into training and validation datasets. Let's fix a seed for PyTorch (to ensure we always get the same validation set), and create the datasets using `random_split`.","metadata":{"id":"mKJ3G3dZGAWz"}},{"cell_type":"code","source":"from torch.utils.data import random_split\n\nrandom_seed = 43\ntorch.manual_seed(random_seed)\n\nval_pct = 0.1\nval_size = int(val_pct * len(dataset))\ntrain_size = len(dataset) - val_size","metadata":{"executionInfo":{"elapsed":789,"status":"ok","timestamp":1610347407884,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"sH8-Jc8qyif3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'm using a validation percentage of 10%, but you can use a smaller or larger percentage. One good strategy is to determine a good set of hyperparameters, and then retrain on a smaller validation set for your final submission.  ","metadata":{"id":"gXi_lt6YINCl"}},{"cell_type":"code","source":"train_ds, valid_ds= random_split(dataset, [train_size, val_size])\nlen(train_ds), len(valid_ds)","metadata":{"executionInfo":{"elapsed":1005,"status":"ok","timestamp":1610347409112,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"4rKV2DDUIdD5","outputId":"a7e8feb5-837a-4e2a-b989-331529725179","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data Loaders:** Next, we can create data loaders for retrieving images in batches.","metadata":{"id":"XTs1npYiIXle"}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nbatch_size = 64\n\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)\nvalid_dl = DataLoader(valid_ds, batch_size*2, num_workers=3, pin_memory=True)","metadata":{"executionInfo":{"elapsed":785,"status":"ok","timestamp":1610347410856,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"v7PzQVSvyybc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.utils import make_grid\n\ndef show_batch(dl):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(12, 12))\n        ax.set_xticks([]); ax.set_yticks([])\n        images = denormalize(images, *stats)\n        ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0))\n        break","metadata":{"executionInfo":{"elapsed":530,"status":"ok","timestamp":1610347411313,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"Yw8YX4l1y6Fk","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_batch(train_dl)","metadata":{"executionInfo":{"elapsed":13769,"status":"ok","timestamp":1610347425350,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"glBU5wdbJMCF","outputId":"a0b72cd8-137a-44b1-d538-ea0bb79eee71","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Using a GPU:** To seamlessly use a GPU, if one is available, we define a couple of helper functions (`get_default_device` & `to_device`) and a helper class `DeviceDataLoader` to move our model & data to the GPU as required.","metadata":{"id":"CamBRyBfVhsj"}},{"cell_type":"code","source":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","metadata":{"executionInfo":{"elapsed":12267,"status":"ok","timestamp":1610347425352,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"2vyu09RnJOmU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on where you're running this notebook, your default device could be a CPU (`torch.device('cpu')`) or a GPU (`torch.device('cuda')`)\n\n","metadata":{"id":"9c6dVjvpWCKi"}},{"cell_type":"code","source":"device = get_default_device()\ndevice","metadata":{"executionInfo":{"elapsed":11406,"status":"ok","timestamp":1610347425354,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"-mP3foLyV8xu","outputId":"46ebe0df-570a-4db2-82ed-6697d7df820f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now wrap our training and validation data loaders using `DeviceDataLoader` for automatically transferring batches of data to the GPU (if available).\n\n","metadata":{"id":"NhCLLxHKWJ3K"}},{"cell_type":"code","source":"train_dl = DeviceDataLoader(train_dl, device)\nvalid_dl = DeviceDataLoader(valid_dl, device)","metadata":{"executionInfo":{"elapsed":10986,"status":"ok","timestamp":1610347425357,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"4BppmuDQWOyJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Model Architecture**\nWe will use the **ResNet9** architecture.\n\nLet's begin with defining the model by extending an ImageClassificationBase class which contains helper methods for training & validation.","metadata":{"id":"mTxp6eLuW9NU"}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\ndef accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch\n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels)  # Calculate loss\n        return loss\n\n    def validation_step(self, batch):\n        images, labels = batch\n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n\n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n\n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}],{} train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, \"last_lr: {:.5f},\".format(result['lrs'][-1]) if 'lrs' in result else '', \n            result['train_loss'], result['val_loss'], result['val_acc']))","metadata":{"executionInfo":{"elapsed":10539,"status":"ok","timestamp":1610347425359,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"ALCkm39AWUAn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's define the ResNet9 model.","metadata":{"id":"wY8wZMUz0Wti"}},{"cell_type":"code","source":"def conv_block(in_channels, out_channels, pool=False):\n    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n              nn.BatchNorm2d(out_channels), \n              nn.ReLU(inplace=True)]\n    if pool: layers.append(nn.MaxPool2d(2))\n    return nn.Sequential(*layers)\n\nclass ResNet9(ImageClassificationBase):\n    def __init__(self, in_channels, num_classes):\n        super().__init__()\n        \n        self.conv1 = conv_block(in_channels, 64)\n        self.conv2 = conv_block(64, 128, pool=True)   \n        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n        \n        self.conv3 = conv_block(128, 256, pool=True)\n        self.conv4 = conv_block(256, 512, pool=True)    \n        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))   \n        \n        self.classifier = nn.Sequential(nn.AdaptiveMaxPool2d(1),\n                                        nn.Flatten(),     \n                                        nn.Dropout(0.2),\n                                        nn.Linear(512, num_classes))    \n        \n    def forward(self, xb):\n        out = self.conv1(xb)\n        out = self.conv2(out)\n        out = self.res1(out) + out\n        out = self.conv3(out)\n        out = self.conv4(out)\n        out = self.res2(out) + out\n        out = self.classifier(out)\n        return out","metadata":{"executionInfo":{"elapsed":718,"status":"ok","timestamp":1610347492365,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"CAWE50NkX1yc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = to_device(ResNet9(3, 5), device)\nmodel","metadata":{"executionInfo":{"elapsed":756,"status":"ok","timestamp":1610347493634,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"B4MnlYCwaaAl","outputId":"46795455-3ac8-4d0d-ecf4-a58aa70087dc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Training the Model**\n\nBefore we train the model, we're going to make a bunch of small but important improvements to our `fit` function:\n\nLet's define a `fit_one_cycle` function to incorporate these changes. We'll also record the learning rate used for each batch.","metadata":{"id":"dZgfzcKTahNz"}},{"cell_type":"code","source":"import torch\nfrom tqdm.notebook import tqdm\n\n@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase\n        model.train()\n        train_losses = []\n        for batch in tqdm(train_loader):\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef fit_one_cycle(epochs, max_lr, model, train_loader, val_loader,\n                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n    torch.cuda.empty_cache()\n    history = []\n\n    # Set up custom optimizer with weight decay\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    # Set up one-cycle learning rate scheduler\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs,\n                                                steps_per_epoch=len(train_loader))\n\n    for epoch in range(epochs):\n        # Training Phase\n        model.train()\n        train_losses = []\n        lrs = []\n        for batch in tqdm(train_loader):\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n\n            # Gradient clipping\n            if grad_clip:\n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n\n            optimizer.step()\n            optimizer.zero_grad()\n\n            # Record & update learning rate\n            lrs.append(get_lr(optimizer))\n            sched.step()\n\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","metadata":{"executionInfo":{"elapsed":877,"status":"ok","timestamp":1610347497531,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"XjWM1lBWabgF","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = [evaluate(model, valid_dl)]\nhistory","metadata":{"executionInfo":{"elapsed":2443,"status":"ok","timestamp":1610347501432,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"e8JDS1XZbbeB","outputId":"59f9340c-9da5-42f5-fe51-284ff7b44204","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We're now ready to train our model. We'll use the Adam optimizer which uses techniques like momentum and adaptive learning rates for faster training.","metadata":{"id":"rbexCGUikBBw"}},{"cell_type":"code","source":"epochs = 10\nmax_lr = 0.01\ngrad_clip = 0.1\nweight_decay = 1e-4\nopt_func = torch.optim.Adam","metadata":{"executionInfo":{"elapsed":729,"status":"ok","timestamp":1610347515943,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"4URTKzxsbmQz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nhistory += fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, \n                             grad_clip=grad_clip, \n                             weight_decay=weight_decay, \n                             opt_func=opt_func)","metadata":{"executionInfo":{"elapsed":155108,"status":"ok","timestamp":1610347671113,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"2zJUoJ62kKrX","outputId":"7a3fcea3-4822-44fb-b6a4-fe05f5da0b52","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_time='2:42'","metadata":{"executionInfo":{"elapsed":911,"status":"ok","timestamp":1610347757312,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"GddqHTEQkLYv","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our model trained to over 76% accuracy in under 3 minutes! \n\nLet's plot the valdation set accuracies to study how the model improves over time.","metadata":{"id":"8ReMIqANkp77"}},{"cell_type":"code","source":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');","metadata":{"executionInfo":{"elapsed":526,"status":"ok","timestamp":1610347757695,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"ehs7zNB6k2ZT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_accuracies(history)","metadata":{"executionInfo":{"elapsed":922,"status":"ok","timestamp":1610347758504,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"c7J5mBw1k5Rk","outputId":"dabe9e52-aa6c-47cf-d891-80fe85583354","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also plot the training and validation losses to study the trend.","metadata":{"id":"1mBrgEbZlCGP"}},{"cell_type":"code","source":"def plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');","metadata":{"executionInfo":{"elapsed":866,"status":"ok","timestamp":1610347762423,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"4df9OZtZk7M0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_losses(history)","metadata":{"executionInfo":{"elapsed":970,"status":"ok","timestamp":1610347762821,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"3lOrpQeRlHgG","outputId":"8e7cea27-df89-435d-9e5c-0d716361f0f0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, let's visualize how the learning rate changed over time, batch-by-batch over all the epochs.","metadata":{"id":"wPJi7vkXlVvW"}},{"cell_type":"code","source":"import numpy as np\n\ndef plot_lrs(history):\n    lrs = np.concatenate([x.get('lrs', []) for x in history])\n    plt.plot(lrs)\n    plt.xlabel('Batch no.')\n    plt.ylabel('Learning rate')\n    plt.title('Learning Rate vs. Batch no.');","metadata":{"executionInfo":{"elapsed":744,"status":"ok","timestamp":1610347769077,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"QzVsi3JrlKFw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_lrs(history)","metadata":{"executionInfo":{"elapsed":823,"status":"ok","timestamp":1610347769476,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"zdZL1kyKlbhC","outputId":"57d8eae9-520b-4024-95ac-d05374cf3c2a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected, the learning rate starts at a low value, and gradually increases for 30% of the iterations to a maximum value of 0.01, and then gradually decreases to a very small value.","metadata":{"id":"BmeORYNLlkjQ"}},{"cell_type":"markdown","source":"## **Testing the Model**\n\nWhile we have been tracking the overall accuracy of a model so far, it's also a good idea to look at model's results on some sample images. Let's test out our model with some images from the predefined validation dataset of 432 images.","metadata":{"id":"3jXl_Y_plxjz"}},{"cell_type":"code","source":"def predict_image(img, model):\n    # Convert to a batch of 1\n    xb = to_device(img.unsqueeze(0), device)\n    # Get predictions from model\n    yb = model(xb)\n    # Pick index with highest probability\n    _, preds  = torch.max(yb, dim=1)\n    # Retrieve the class label\n    return dataset.classes[preds[0].item()]","metadata":{"executionInfo":{"elapsed":756,"status":"ok","timestamp":1610347773337,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"EHIiOI60lgS8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img, label = valid_ds[6]\nplt.imshow(img.permute(1, 2, 0).clamp(0, 1))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","metadata":{"executionInfo":{"elapsed":955,"status":"ok","timestamp":1610347774092,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"AMJwPg5yl-su","outputId":"9e50410f-1b37-46db-cd99-f40cf169b1a4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img, label = valid_ds[156]\nplt.imshow(img.permute(1, 2, 0).clamp(0, 1))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","metadata":{"executionInfo":{"elapsed":740,"status":"ok","timestamp":1610347776007,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"b8OkIRZimB8u","outputId":"29c6d033-6b91-43b7-ad7b-cc9501d0cff9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img, label = valid_ds[388]\nplt.imshow(img.permute(1, 2, 0).clamp(0, 1))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","metadata":{"executionInfo":{"elapsed":749,"status":"ok","timestamp":1610347780603,"user":{"displayName":"Rayan Kazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4rttzfFZaIjsNIJ9CQT5LpdPag5TCep3a3qkHsg=s64","userId":"04315729004695110220"},"user_tz":-180},"id":"td3NMqb4oUO4","outputId":"9c026264-0b3e-42a5-bbf4-baa257d0f963","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Identifying where our model performs poorly can help us improve the model, by collecting more training data, increasing/decreasing the complexity of the model, and changing the hypeparameters.","metadata":{"id":"EauE86C1olN7"}},{"cell_type":"markdown","source":"## **Summary**  \nHere's a summary of the different techniques used in this tutorial to improve our model performance and reduce the training time:\n\n- **Data normalization:** We normalized the image tensors by subtracting the mean and dividing by the standard deviation of pixels across each channel. Normalizing the data prevents the pixel values from any one channel from disproportionately affecting the losses and gradients. [Learn more](https://medium.com/@ml_kid/what-is-transform-and-transform-normalize-lesson-4-neural-networks-in-pytorch-ca97842336bd)\n\n- **Data augmentation:** We applied random transformations while loading images from the training dataset. Specifically, we will resize the images to 64 x 64 pixels, then we will randomly crop the image to 64 x 64 pixels with padding by 4 pixels, and then flip the image horizontally with a 50% probability. [Learn more](https://www.analyticsvidhya.com/blog/2019/12/image-augmentation-deep-learning-pytorch/)\n\n- **Residual connections:** One of the key changes to our CNN model was the addition of the resudial block, which adds the original input back to the output feature map obtained by passing the input through one or more convolutional layers. We used the ResNet9 architecture. [Learn more](https://towardsdatascience.com/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec?gi=68b790ca041)\n\n- **Batch normalization:** After each convolutional layer, we added a batch normalization layer, which normalizes the outputs of the previous layer. This is somewhat similar to data normalization, except it's applied to the outputs of a layer, and the mean and standard deviation are learned parameters. [Learn more](https://towardsdatascience.com/batch-normalization-and-dropout-in-neural-networks-explained-with-pytorch-47d7a8459bcd)\n\n- **Learning rate scheduling:** Instead of using a fixed learning rate, we will use a learning rate scheduler, which will change the learning rate after every batch of training. There are many strategies for varying the learning rate during training, and we used the \"One Cycle Learning Rate Policy\". [Learn more](https://sgugger.github.io/the-1cycle-policy.html)\n\n- **Weight Decay:** We added weight decay to the optimizer, yet another regularization technique which prevents the weights from becoming too large by adding an additional term to the loss function. [Learn more](https://towardsdatascience.com/this-thing-called-weight-decay-a7cd4bcfccab)\n\n- **Gradient clipping:** We also added gradient clippint, which helps limit the values of gradients to a small range to prevent undesirable changes in model parameters due to large gradient values during training. [Learn more](https://towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48#63e0)\n\n- **Adam optimizer:** Instead of SGD (stochastic gradient descent), we used the Adam optimizer which uses techniques like momentum and adaptive learning rates for faster training. There are many other optimizers to choose froma and experiment with. [Learn more](https://ruder.io/optimizing-gradient-descent/index.html)","metadata":{"id":"CpOsiA3ZqHpB"}},{"cell_type":"code","source":"","metadata":{"id":"MUH24nelqEMd"},"execution_count":null,"outputs":[]}]}